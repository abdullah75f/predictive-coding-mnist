\documentclass[a4paper,12pt]{article}
\usepackage{amsmath}
\usepackage{geometry}
\geometry{margin=1in}
\usepackage{parskip}
\usepackage{noto}

\begin{document}

\title{Predictive Coding Model for MNIST Classification}
\author{Abdullah Farid}
\date{June 18, 2025}
\maketitle

\section{Introduction}
Predictive coding is a computational framework inspired by neuroscience, positing that the brain actively predicts sensory inputs rather than passively processing them. It refines its internal model by learning from prediction errorsâ€”the mismatch between expectation and reality. This report outlines the successful implementation of a hierarchical predictive coding model for the task of MNIST image classification. The model incorporates both top-down predictions and lateral connections within hidden layers to model feature interactions. The logical flow from neuroscientific theory to a stable, working Python implementation is detailed, covering the model's architecture, its unique learning dynamics, and a final evaluation of its performance.

\section{Predictive Coding Theory}
The core tenet of predictive coding is the minimization of prediction error (or "surprise") throughout a neural hierarchy. The system operates through two distinct but interconnected mechanisms:

\begin{itemize}
    \item \textbf{Inference:} The rapid adjustment of neural activity (\textbf{states}, $x_i$) to find the most likely cause of the current sensory input. This process minimizes immediate prediction error.
    \item \textbf{Learning:} The slow adjustment of synaptic connections (\textbf{weights}, $W_i$) to improve the accuracy of future predictions.
\end{itemize}

In our hierarchical model, a higher layer $i+1$ generates a top-down prediction of the activity in the layer below, $i$. The prediction error $e_i$ is the difference between the actual state $x_i$ and its prediction $\hat{x}_i$. These errors serve as the primary drivers for both inference and learning.

Key equations governing the dynamics are:
\begin{enumerate}
    \item \textbf{Prediction:} The prediction of layer $i$'s state is generated from layer $i+1$: 
    \[ \hat{x}_i = f(W_i x_{i+1} + b_i) \]
    where $f$ is a non-linear activation function (ReLU in our case).

    \item \textbf{Prediction Error:} The error at layer $i$ is the discrepancy between its state and its prediction:
    \[ e_i = x_i - \hat{x}_i \]

    \item \textbf{State Update (Inference):} The state of a hidden layer neuron $x_i$ is updated to reduce error from both above and below. The change in state, $\Delta x_i$, is driven by a bottom-up signal and a top-down signal:
    \[ \Delta x_i \propto (W_{i-1}^T e_{i-1}) - e_i \]
    
    \item \textbf{Weight Update (Learning):} Synaptic weights are updated according to a Hebbian-like rule, based on the final settled states and errors:
    \[ \Delta W_i \propto e_i \cdot f(x_{i+1})^T \]
\end{enumerate}

\section{Model Design and Implementation}
The model is implemented as a Python class, \texttt{PredictiveCodingModel}, using NumPy for computations and PyTorch for the MNIST data pipeline.

\subsection{Architecture}
The network has four layers: an input layer (784 nodes for a flattened 28x28 image), two hidden layers (256 and 64 nodes), and an output layer (10 nodes, one per digit class).
\begin{itemize}
    \item \textbf{Forward Weights ($W_i$):} Connect adjacent layers and are responsible for generating top-down predictions.
    \item \textbf{Lateral Weights ($W_{\text{lateral}}$):} Exist within each hidden layer, allowing neurons to interact and mutually influence their states. This encourages competition and the formation of feature assemblies.
    \item \textbf{States ($x_i$):} Represent the current activation of neurons in each layer.
    \item \textbf{Errors ($e_i$):} Represent the prediction error at each layer.
\end{itemize}

\subsection{The Learning Process: From Theory to Practice}
Translating the theory into a stable learning algorithm required several key implementation details, discovered through iterative development. The training process for a single sample is orchestrated by the \texttt{train()} method and consists of three distinct phases.

\textbf{Phase 0: Providing a Teacher Signal via Clamping}
For supervised classification, the model must be informed of the correct answer. This is achieved by \textbf{clamping} the output layer: the state vector of the final layer, $x_{\text{out}}$, is forcibly set to the one-hot encoded vector of the true label and is held constant throughout the inference phase. This provides a strong, unambiguous error signal that propagates down to the hidden layers.

\textbf{Phase 1: Inference via State Settling}
With the input layer holding the image and the output layer clamped, the model enters an iterative inference loop for a fixed number of steps (e.g., 50 iterations):
\begin{enumerate}
    \item \texttt{forward()}: Top-down predictions are generated at all layers.
    \item \texttt{compute\_errors()}: Prediction errors are calculated. To ensure numerical stability and prevent exploding dynamics, these errors are \textbf{clipped} to a fixed range [-1.0, 1.0].
    \item \texttt{update\_states()}: The states of the hidden layers ($x_1, x_2$) are updated according to the dynamics described in Section 2. The clamped output layer is not updated. This loop allows the hidden states to settle into a configuration that best reconciles the sensory input with the clamped (correct) output.
\end{enumerate}

\textbf{Phase 2: Learning via Weight Updates}
After the inference loop completes, the \texttt{update\_weights()} method is called \textbf{once}.